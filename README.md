# NTTV Chatbot â€” Deterministic RAG Assistant for Ninja Training TV

A **RAG-based**, **extractor-driven**, and **deterministic** chatbot for Ninja Training TV (NTTV).  
Built in Python using **FAISS**, **sentence-transformers**, **Streamlit**, and a suite of custom **extractors** for rank, kihon, sanshin, schools, weapons, kyusho, and more.

Runs **locally** or in the **cloud (Render)** with the same index + retrieval pipeline.

---

## ğŸš€ Key Features

### ğŸ§  Deterministic Knowledge Layer

- Extractors for:
  - **Rank requirements**
  - **Kihon HappÅ**
  - **Sanshin no Kata**
  - **Schools (RyÅ«ha)**
  - **Weapons**
  - **Kyusho**
- Hard-coded, rank-aware responses where appropriate.
- Zero hallucinations for strict/deterministic queries when extractors fire.

### ğŸ” RAG Retrieval Engine

- **FAISS** vector index
- **Sentence-Transformers** embeddings (`all-MiniLM-L6-v2`)
- Priority-aware reranking:
  - **P1**: Rank files
  - **P2**: Techniques / schools / kihon / weapons
  - **P3**: Other passages
- Adjustable **TOP_K** and fallback heuristics.

### ğŸ’¬ Streamlit App UI

- Question input + answer display
- **Debug mode** (shows top passages, raw model response)
- **Explanation mode** (short fact â†’ brief rationale)
- **Technique detail level** (Brief / Standard / Full)
- Source citations and passage inspection

---

## ğŸ“¦ Repository Structure

    nttv_chatbot_ext/
    â”‚
    â”œâ”€â”€ app.py                 # Streamlit UI + RAG pipeline
    â”œâ”€â”€ ingest.py              # Builds FAISS index + meta.pkl from /data
    â”œâ”€â”€ extractors/            # Deterministic extractors (rank, kihon, weapons, etc.)
    â”œâ”€â”€ data/                  # Authoritative text sources
    â”œâ”€â”€ index/                 # Local FAISS index artifacts (created by ingest.py)
    â”œâ”€â”€ tests/                 # Pytest suite + prompt harness
    â”œâ”€â”€ requirements.txt       # Python dependencies
    â”œâ”€â”€ render.yaml            # Render Blueprint for cloud deployment
    â””â”€â”€ README.md              # You are here

---

## ğŸ›  Installation (Local)

### 1. Clone the repo

    git clone https://github.com/paulzim/nttv_chatbot_ext
    cd nttv_chatbot_ext

### 2. Create a virtual environment

macOS / Linux:

    python -m venv .venv
    source .venv/bin/activate

Windows (PowerShell):

    python -m venv .venv
    .\.venv\Scripts\activate

### 3. Install dependencies

    pip install -U pip
    pip install -r requirements.txt

### 4. Build the FAISS index

    python ingest.py

This reads all files in `data/`, chunks them, embeds them, and writes:

- `index/faiss.index`
- `index/meta.pkl`
- `index/config.json`

### 5. Run the chatbot

    streamlit run app.py

Then open the provided URL (typically `http://localhost:8501`).

---

## âš™ï¸ Environment Variables

Used locally (via `.env`) and in the cloud (via Render).

| Variable              | Example                                         | Purpose                               |
|-----------------------|-------------------------------------------------|---------------------------------------|
| `OPENAI_BASE_URL`     | `https://openrouter.ai/api/v1`                 | Endpoint for model inference          |
| `OPENAI_API_KEY`      | `sk-or-...`                                    | API key (keep secret)                 |
| `MODEL_NAME`          | `openrouter/anthropic/claude-3.5-sonnet`       | LLM identifier                        |
| `EMBED_MODEL_NAME`    | `sentence-transformers/all-MiniLM-L6-v2`       | Embedding model for FAISS             |
| `INDEX_DIR`           | `index/`                                       | Index directory root                  |
| `INDEX_PATH`          | `index/faiss.index`                            | FAISS index file path                 |
| `META_PATH`           | `index/meta.pkl`                               | Metadata (retrieval chunks)           |
| `RANK_FILE`           | `data/nttv rank requirements.txt`              | Rank source of truth                  |
| `TOP_K`               | `6`                                            | Retrieval depth                       |
| `TEMPERATURE`         | `0.0`                                          | Deterministic output                  |
| `MAX_TOKENS`          | `512`                                          | Generation token cap                  |
| `STREAMLIT_BROWSER_GATHER_USAGE_STATS` | `false`                      | Disable Streamlit telemetry           |

### Example `.env` (local)

    OPENAI_BASE_URL=https://openrouter.ai/api/v1
    OPENAI_API_KEY=sk-or-xxxx
    MODEL_NAME=openrouter/anthropic/claude-3.5-sonnet

    EMBED_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
    INDEX_DIR=index
    INDEX_PATH=index/faiss.index
    META_PATH=index/meta.pkl
    RANK_FILE=data/nttv rank requirements.txt

    TOP_K=6
    TEMPERATURE=0.0
    MAX_TOKENS=512
    STREAMLIT_BROWSER_GATHER_USAGE_STATS=false

> âš ï¸ Do **not** commit `.env` or real secrets to git.

---

## ğŸ§ª Testing

Run the full test suite:

    pytest -q

Includes:

- Extractor tests (rank, kihon, sanshin, weapons, schools, etc.)
- Retrieval overlap / consistency checks
- Prompt harness using real rank file as first passage
- Technique normalization validations

You can add new prompt tests under:

    tests/prompts/

---

## â˜ï¸ Deploying to Render

### 1. Ensure `render.yaml` is at the repo root

Render uses this as a Blueprint for the service.

### 2. In Render

- New â†’ â€œBlueprintâ€ â†’ connect this repo.
- Confirm `buildCommand` runs:

    pip install -U pip && pip install -r requirements.txt && python ingest.py

- Confirm `startCommand` is:

    streamlit run app.py --server.port $PORT --server.address 0.0.0.0

- Add environment variables in the Render dashboard:
  - `OPENAI_API_KEY`
  - `OPENAI_BASE_URL`
  - `MODEL_NAME`
  - Any overrides for `INDEX_DIR`, `TOP_K`, etc.

### 3. Index rebuilds

Whenever files in `data/` change:

- Push a new commit â†’ Render rebuilds and re-runs `python ingest.py`, or
- Use â€œManual Deploy â†’ Clear build cache & deployâ€ to force a fresh ingest.

---

## ğŸ”§ Common Issues

### â€œIndex config / meta not foundâ€

- Make sure `python ingest.py` ran successfully.
- Verify that `index/config.json` and `index/meta.pkl` exist.
- Check `INDEX_DIR`, `INDEX_PATH`, and `META_PATH` in:
  - Local `.env`, and/or
  - Renderâ€™s environment settings.

### FAISS index issues

- Ensure `faiss-cpu` is installed (see `requirements.txt`).
- Confirm `faiss.index` path matches `INDEX_PATH` (or config.jsonâ€™s `faiss_path`).

### LLM errors (401/403/429)

- Check `OPENAI_API_KEY` validity and scope.
- Verify `OPENAI_BASE_URL` is correct (including `/v1` suffix).
- Make sure `MODEL_NAME` is available to your key.

### Slow or 5xx responses on Render

- Free tier may sleep and add startup latency.
- Upgrade to a plan with more RAM/CPU for smoother RAG + Streamlit.
- Use Streamlitâ€™s debug mode to inspect retrieval / model timings.

---

## ğŸ§­ Roadmap

- Add deterministic extractor for **Kyusho**.
- Expand schools and weapons metadata.
- Add `/healthz` or similar health check endpoint.
- Provide a simple `deploy.sh` for DigitalOcean / VPS targets.
- Grow the prompt harness with rank- and weapon-specific test cases.

---

## ğŸ“œ License

MIT License â€” free for personal or commercial use.

---

## ğŸ™ Credits

Built with:

- Streamlit  
- FAISS  
- Sentence-Transformers  
- OpenRouter / OpenAI-compatible APIs  
- And a lot of Bujinkan / NTTV curriculum work
